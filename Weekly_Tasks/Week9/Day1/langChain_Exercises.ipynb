{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45fbb33b",
      "metadata": {
        "id": "45fbb33b"
      },
      "source": [
        "# LangChain Tutorial Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a0df42",
      "metadata": {
        "id": "34a0df42"
      },
      "source": [
        "\n",
        "In this notebook, you will practice using LangChain to interact with large language models (LLMs),\n",
        "build chains, agents, and utilize memory. Fill in the code blocks with your implementations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "169dd23c",
      "metadata": {
        "id": "169dd23c"
      },
      "source": [
        "## Exercise 1: Basic LLM Query"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf03f4a7",
      "metadata": {
        "id": "bf03f4a7"
      },
      "source": [
        "In this exercise, you will set up a basic interaction with the GROQ LLaMA model using LangChain.\n",
        "\n",
        "1. Initialize the LLM (Use GROQ and chose LLM).\n",
        "2. Create a prompt that asks the LLM to generate a story about a topic.\n",
        "3. Run the LLM chain to retrieve the response.\n",
        "\n",
        "**Steps**:\n",
        "- Import required modules from `langchain`.\n",
        "- Initialize the LLM with your GROQ API key.\n",
        "- Create a prompt template that takes a topic as input.\n",
        "- Create an LLM Chain and run it to get a response.\n",
        "\n",
        "Fill in the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5686ae2f",
      "metadata": {
        "id": "5686ae2f"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-community langchain-groq duckduckgo-search geopy requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "groq_api_key = \"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\""
      ],
      "metadata": {
        "id": "GRkWux9ASWPN"
      },
      "id": "GRkWux9ASWPN",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.1-8b-instant\",\n",
        "    temperature=0.8,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    api_key=groq_api_key\n",
        ")"
      ],
      "metadata": {
        "id": "7wOQCbe5Ss8p"
      },
      "id": "7wOQCbe5Ss8p",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=\"Write a short story about {topic}.\"\n",
        ")"
      ],
      "metadata": {
        "id": "2gIGj5fDSzfE"
      },
      "id": "2gIGj5fDSzfE",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create an LLM chain\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run the chain with a specific topic\n",
        "response = chain.run(\"Game of thrones\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4MGhcYxS5fB",
        "outputId": "c5e295e5-5485-422f-ebf1-728d2f8d1c95"
      },
      "id": "V4MGhcYxS5fB",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-28658761d408>:4: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt)\n",
            "<ipython-input-5-28658761d408>:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
            "  response = chain.run(\"Game of thrones\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**The Price of Loyalty**\n",
            "\n",
            "The snow fell gently over the fortress of Winterfell, casting a serene silence over the castle. But amidst the peacefulness, Arya Stark walked the cold stone halls, her eyes fixed on the floor. She had just received news from a trusted messenger - Jon Snow had been killed in battle, fighting to reclaim the North from the Night King.\n",
            "\n",
            "Arya's heart ached as she recalled the countless times Jon had saved her life, protected her, and fought for her family. She had grown to consider him a brother, and his passing left a gaping hole in her world.\n",
            "\n",
            "Determined to honor his memory, Arya set out on a perilous journey to find the man responsible for Jon's death. She traveled across the Seven Kingdoms, navigating treacherous landscapes and blood-thirsty foes. Along the way, she encountered old friends and foes, including a certain Sand Snake, who offered her guidance and support.\n",
            "\n",
            "As Arya drew closer to her target, she encountered a familiar face - Brienne of Tarth, the Kingslayer. The noble knight had been a thorn in Arya's side for years, but now, in this moment of grief, she saw an opportunity to join forces and avenge Jon's death.\n",
            "\n",
            "Together, they tracked the treacherous Lord of Winterfell, betraying the Starks and joining the Night King's army. His name was Ramsay Bolton's cousin, cutting brother, and master of the sadistic darkness that had taken Jon's life. The final battle was fierce, with Arya and Brienne fighting side by side against the enemy forces.\n",
            "\n",
            "In the heat of the battle, Arya confronted her nemesis, finally facing the man who had killed Jon Snow. With a fierce cry, she plunged her Valyrian steel dagger into his chest, ending his reign of terror. As she stood victorious, she felt a sense of closure, but also a deep sadness for the loss of her beloved brother.\n",
            "\n",
            "Brienne approached her, a gentle smile on her face. \"Arya, you've done it. You've avenged Jon's death.\"\n",
            "\n",
            "Arya looked up at her, her eyes wet with tears. \"I had to. He was my brother, my friend. I couldn't let him down.\"\n",
            "\n",
            "Brienne nodded, placing a hand on Arya's shoulder. \"You didn't let him down, Arya. You made him proud.\"\n",
            "\n",
            "And in that moment, surrounded by the snow-covered walls of Winterfell, Arya felt a sense of peace wash over her. She had honored Jon's memory, and in doing so, she had found a piece of herself. The price of loyalty had been high, but it was one she was willing to pay, time and time again.\n",
            "\n",
            "---\n",
            "\n",
            "This short story is a tribute to the beloved characters and world of Game of Thrones, and is not connected to any specific episode or plotline. It is a standalone story that explores the themes of loyalty, sacrifice, and the power of friendship.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff1b3fa",
      "metadata": {
        "id": "1ff1b3fa"
      },
      "source": [
        "## Exercise 2: Building a Conversational Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a6a1f26",
      "metadata": {
        "id": "4a6a1f26"
      },
      "source": [
        "\n",
        "In this exercise, you will create a conversational agent that can interact with a user, make decisions,\n",
        "and use external tools like a search tool.\n",
        "\n",
        "1. Define a tool.\n",
        "2. Create an agent that can decide whether to use the tool or interact with the LLM.\n",
        "3. Run the agent with various inputs.\n",
        "\n",
        "**Steps**:\n",
        "- Define the search tool using a function.\n",
        "- Initialize an agent using the tool and the LLM.\n",
        "- Run the agent with sample inputs.\n",
        "\n",
        "Fill in the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a6e3e8e2",
      "metadata": {
        "id": "a6e3e8e2"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools import DuckDuckGoSearchRun\n",
        "\n",
        "search = DuckDuckGoSearchRun()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "tools = [search]\n",
        "\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeJ7ZXfMTGTc",
        "outputId": "cd410460-f4b9-46b0-9f61-36120fb8c70f"
      },
      "id": "OeJ7ZXfMTGTc",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-ade465319c73>:5: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 1.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
            "  agent = initialize_agent(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"Find information about game of thrones.\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN_dKatkTSWF",
        "outputId": "4904c3b4-7fc3-463e-e5ed-8503ab8b4f47"
      },
      "id": "oN_dKatkTSWF",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Game of Thrones series is an American fantasy drama TV series based on George R. R. Martin's novels, following the political and military conflicts among noble families vying for the Iron Throne of Westeros and the threat of the winter beyond the Wall.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cf94d2f",
      "metadata": {
        "id": "3cf94d2f"
      },
      "source": [
        "## Exercise 3: Using LLM as Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ff48d9",
      "metadata": {
        "id": "53ff48d9"
      },
      "source": [
        "\n",
        "In this exercise, you will use an LLM to summarize and retain information from conversations.\n",
        "\n",
        "1. Set up LLM-based memory.\n",
        "2. Create a conversation with the LLM and memory.\n",
        "3. Ask follow-up questions using memory to retrieve past context.\n",
        "\n",
        "**Steps**:\n",
        "- Initialize summarization-based memory.\n",
        "- Run a few queries and retrieve responses.\n",
        "- Ask follow-up questions that reference previous interactions.\n",
        "\n",
        "Fill in the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "796557b9",
      "metadata": {
        "id": "796557b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a15b1b-7ad4-4fe6-84cb-db07ff1942b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e6e306a6b8d0>:11: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
            "  chain_with_buffer_memory = ConversationChain(llm=llm, memory=summary_memory)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "from langchain import ConversationChain\n",
        "\n",
        "# Initialize summarization-based memory\n",
        "summary_memory = ConversationSummaryMemory(llm=llm)\n",
        "\n",
        "# Clear memory\n",
        "summary_memory.clear()\n",
        "\n",
        "# Create a chain with LLM and buffer memory\n",
        "chain_with_buffer_memory = ConversationChain(llm=llm, memory=summary_memory)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start a conversation\n",
        "res1 = chain_with_buffer_memory.run(\"Answer with short answer, give me a number between (0 and 9)\")\n",
        "res2 = chain_with_buffer_memory.run(\"Answer with short answer, give me another number between (10 and 19)\")\n",
        "res3 = chain_with_buffer_memory.run(\"From the previous conversations. What was these numbers and what is the result of adding them ?\")\n",
        "\n",
        "print(f'First Response: {res1}\\nSecond Response: {res2}\\nThird Response: {res3}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_AfK9o_Tgq_",
        "outputId": "5f0e2ab3-11e9-4b91-8996-c5e94f59d4b0"
      },
      "id": "J_AfK9o_Tgq_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Response: 5\n",
            "Second Response: Another number between 10 and 19. Since I need to provide a random number within this range, I'll generate it based on the uniform distribution. I'll also make sure it's not the same number I previously provided, which was 5.\n",
            "\n",
            "The random number between 10 and 19 is... 17.\n",
            "\n",
            "So, my short answer is: 17.\n",
            "Third Response: From the previous conversations, we have two numbers: 5 and 17. To find the result of adding them, I'll perform the arithmetic operation: 5 + 17 = 22.\n",
            "\n",
            "So, the result of adding these two numbers is 22.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "630fe2fb",
      "metadata": {
        "id": "630fe2fb"
      },
      "source": [
        "## Exercise 4: Combining Tools and Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17dba76c",
      "metadata": {
        "id": "17dba76c"
      },
      "source": [
        "\n",
        "In this final exercise, you will build an intelligent agent that can use both tools (like an API) and memory.\n",
        "\n",
        "1. Define an external tool (like a weather API).\n",
        "2. Set up an agent that uses both the tool and LLM memory.\n",
        "3. Interact with the agent, combining memory and external data.\n",
        "\n",
        "**Steps**:\n",
        "- Define a weather API tool (mock or real API).\n",
        "- Initialize the agent with memory and the tool.\n",
        "- Run the agent with inputs, and check how it uses both memory and tools.\n",
        "\n",
        "Fill in the code below:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "5e11ad68",
      "metadata": {
        "id": "5e11ad68"
      },
      "outputs": [],
      "source": [
        "from geopy.geocoders import Nominatim\n",
        "geolocator = Nominatim(user_agent=\"LLMexercise\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "city_name = 'Madinah Saudi Arabia '\n",
        "location = geolocator.geocode(city_name)\n",
        "location"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPUDDRrGT-b7",
        "outputId": "5f474af5-0596-49d2-8696-946883091405"
      },
      "id": "hPUDDRrGT-b7",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Location(المدينة المنورة, محافظة المدينة المنورة, منطقة المدينة المنورة, 41419, السعودية, (24.471153, 39.6111216, 0.0))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "latitude = location.latitude\n",
        "longitude = location.longitude\n",
        "\n",
        "print(f'Latitude: {latitude}, longitude: {longitude}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foeGPA9DUCnx",
        "outputId": "207e1e87-8939-45e4-f413-29f7711b3c09"
      },
      "id": "foeGPA9DUCnx",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latitude: 24.471153, longitude: 39.6111216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from geopy.geocoders import Nominatim\n",
        "\n",
        "def get_weather_by_city(city_name: str):\n",
        "    # Initialize geocoder\n",
        "    geolocator = Nominatim(user_agent=\"LLMexercise\")\n",
        "\n",
        "    # Get location data (latitude and longitude) for the city\n",
        "    location = geolocator.geocode(city_name)\n",
        "\n",
        "    if location:\n",
        "        latitude = location.latitude\n",
        "        longitude = location.longitude\n",
        "    else:\n",
        "        return f\"City '{city_name}' not found.\"\n",
        "\n",
        "    # Open-Meteo API endpoint with the obtained latitude and longitude\n",
        "    url = f\"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current_weather=true\"\n",
        "\n",
        "    # Send a GET request to the Open-Meteo API\n",
        "    response = requests.get(url)\n",
        "\n",
        "    # Check if the request was successful\n",
        "    if response.status_code == 200:\n",
        "        # Parse the JSON data from the response\n",
        "        weather_data = response.json()\n",
        "\n",
        "        # Extract relevant information from the response\n",
        "        current_weather = weather_data.get('current_weather', {})\n",
        "        temperature = current_weather.get('temperature')\n",
        "        windspeed = current_weather.get('windspeed')\n",
        "        winddirection = current_weather.get('winddirection')\n",
        "        weather_time = current_weather.get('time')\n",
        "\n",
        "        # Return the weather information as a formatted string\n",
        "        return (\n",
        "            f\"Current weather in {city_name}:\\n\"\n",
        "            f\"Temperature: {temperature}°C\\n\"\n",
        "            f\"Wind Speed: {windspeed} m/s\\n\"\n",
        "            f\"Wind Direction: {winddirection}°\\n\"\n",
        "            f\"Time of data: {weather_time}\"\n",
        "        )\n",
        "    else:\n",
        "        return \"Failed to retrieve weather data.\"\n",
        "\n",
        "city_name = \"Madinah Saudi Arabia\"\n",
        "weather_info = get_weather_by_city(city_name)\n",
        "print(weather_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIYcUA_sUFSz",
        "outputId": "47baae12-969d-494d-e69d-0d7218f3734a"
      },
      "id": "DIYcUA_sUFSz",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current weather in Madinah Saudi Arabia:\n",
            "Temperature: 38.5°C\n",
            "Wind Speed: 3.3 m/s\n",
            "Wind Direction: 276°\n",
            "Time of data: 2024-09-17T09:30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import Tool\n",
        "\n",
        "weather = Tool(\n",
        "    name=\"weather\",\n",
        "    func=get_weather_by_city,\n",
        "    description=\"Gets the weather of a city.\"\n",
        ")"
      ],
      "metadata": {
        "id": "sU_MeTOMUNoC"
      },
      "id": "sU_MeTOMUNoC",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import initialize_agent, AgentType\n",
        "\n",
        "# List of tools the agent can use\n",
        "tools = [weather]\n",
        "\n",
        "# Initialize the agent\n",
        "agent = initialize_agent(\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
        ")"
      ],
      "metadata": {
        "id": "8p6uIWRRUTBA"
      },
      "id": "8p6uIWRRUTBA",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run(\"What is the weather in Madinah Saudi Arabia?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAXA7x3DUVDt",
        "outputId": "6c739a15-3960-4d27-bccd-5ccf9540b38a"
      },
      "id": "YAXA7x3DUVDt",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The weather in Madinah Saudi Arabia is:\n",
            "Current weather in Madinah Saudi Arabia:\n",
            "Temperature: 38.5°C\n",
            "Wind Speed: 3.3 m/s\n",
            "Wind Direction: 276°\n",
            "Time of data: 2024-09-17T09:30\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}